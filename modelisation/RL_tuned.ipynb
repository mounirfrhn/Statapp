{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mounir/Desktop/PROJETS/Statapp/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "from sklearn.metrics import auc, roc_curve, precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"criteo/FairJob\")\n",
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['click','user_id','impression_id','product_id'])\n",
    "y = df['click']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_log_loss_by_class(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Calcule la log loss globale, la log loss par classe, et la confiance moyenne sur les vrais positifs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Vecteur des vraies √©tiquettes (0 ou 1)\n",
    "    y_proba : array-like, shape (n_samples, 2)\n",
    "        Probabilit√©s pr√©dites (sorties de predict_proba)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "        - 'log_loss_global'\n",
    "        - 'log_loss_y0'\n",
    "        - 'log_loss_y1'\n",
    "        - 'mean_proba_y1' (moyenne des p(y=1) quand y=1)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_proba = np.array(y_proba)\n",
    "\n",
    "    return {\n",
    "        \"log_loss_global\": round(log_loss(y_true, y_proba), 5),\n",
    "        \"log_loss_y0\": round(log_loss(y_true[y_true == 0], y_proba[y_true == 0], labels=[0, 1]), 5),\n",
    "        \"log_loss_y1\": round(log_loss(y_true[y_true == 1], y_proba[y_true == 1], labels=[0, 1]), 5),\n",
    "        \"mean_proba_y1\": round(float(y_proba[y_true == 1, 1].mean()), 5)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no class_weight, no scaling\n",
      "\n",
      "Training done!\n",
      " R√©sultats log loss :\n",
      " {'log_loss_global': 0.55741, 'log_loss_y0': 0.55746, 'log_loss_y1': 0.55016, 'mean_proba_y1': 0.62599}\n"
     ]
    }
   ],
   "source": [
    "print(\"no class_weight, no scaling\")\n",
    "\n",
    "cat_cols_bin = list(X_train.columns[0:3])       \n",
    "rank_col = ['rank']                             \n",
    "cat_cols = list(X_train.columns[4:17])      \n",
    "\n",
    "data_preproc = ColumnTransformer([('multicat_encoding', TargetEncoder(), cat_cols)], \n",
    "                              remainder='passthrough', force_int_remainder_cols=False)\n",
    "\n",
    "logreg_pipe = Pipeline([('preprocessing', data_preproc),\n",
    "                     ('logreg',LogisticRegression())\n",
    "                     ])\n",
    "\n",
    "logreg_param_dict = {\n",
    "    'logreg__C': 1.0,\n",
    "    'logreg__penalty': \"l2\",\n",
    "    'logreg__class_weight': \"balanced\",\n",
    "    'logreg__max_iter': 1000,\n",
    "    'logreg__solver': \"lbfgs\",\n",
    "    'logreg__random_state': 42\n",
    "}\n",
    "\n",
    "logreg_pipe.set_params(**logreg_param_dict)\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_prob = logreg_pipe.predict_proba(X_test)\n",
    "print(\"\\nTraining done!\")\n",
    "\n",
    "results = evaluate_log_loss_by_class(y_test, y_prob)\n",
    "print(\" R√©sultats log loss :\\n\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight = balanced, scaling\n",
      "\n",
      "Training done!\n",
      " R√©sultats log loss :\n",
      " {'log_loss_global': 0.51302, 'log_loss_y0': 0.51212, 'log_loss_y1': 0.64067, 'mean_proba_y1': 0.59398}\n"
     ]
    }
   ],
   "source": [
    "print(\"class_weight = balanced, scaling\")\n",
    "\n",
    "cat_cols_bin = list(X_train.columns[0:3])       \n",
    "rank_col = ['rank']                             \n",
    "cat_cols = list(X_train.columns[4:17])      \n",
    "num_cols = [col for col in X_train.columns if col not in cat_cols + cat_cols_bin + rank_col]\n",
    "\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    TargetEncoder(),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "data_preproc = ColumnTransformer([\n",
    "    ('cat_preproc', cat_pipeline, cat_cols),\n",
    "    ('num_scaling', StandardScaler(), num_cols)\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "logreg_pipe = Pipeline([\n",
    "    ('preprocessing', data_preproc),\n",
    "    ('logreg', logreg_model)\n",
    "])\n",
    "\n",
    "logreg_param_dict = {\n",
    "    'logreg__C': 1.0,\n",
    "    'logreg__penalty': \"l2\",\n",
    "    'logreg__class_weight': \"balanced\",\n",
    "    'logreg__max_iter': 1000,\n",
    "    'logreg__solver': \"lbfgs\",\n",
    "    'logreg__random_state': 42\n",
    "}\n",
    "\n",
    "logreg_pipe.set_params(**logreg_param_dict)\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_prob = logreg_pipe.predict_proba(X_test)\n",
    "print(\"\\nTraining done!\")\n",
    "\n",
    "results = evaluate_log_loss_by_class(y_test, y_prob)\n",
    "print(\" R√©sultats log loss :\\n\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214446, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Click Rank Utility': np.float64(0.01384),\n",
       " 'Negative Log-Likelihood': np.float64(0.51302),\n",
       " 'AUC': np.float64(0.78448),\n",
       " 'Demographic Parity': np.float64(-3e-05)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions import report_results\n",
    "\n",
    "report_results(df, y_prob[:,1], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-13 19:35:52,907] A new study created in memory with name: no-name-c8cff204-efa1-4bc7-90ae-5af458da84a1\n",
      "[I 2025-04-13 19:36:15,838] Trial 0 finished with value: 0.502858995153776 and parameters: {'C': 3.8995702850293155, 'max_iter': 365}. Best is trial 0 with value: 0.502858995153776.\n",
      "[I 2025-04-13 19:36:38,906] Trial 1 finished with value: 0.5026418487340943 and parameters: {'C': 0.1082698616271391, 'max_iter': 1573}. Best is trial 1 with value: 0.5026418487340943.\n",
      "[I 2025-04-13 19:37:00,920] Trial 2 finished with value: 0.502675714735436 and parameters: {'C': 0.1522584680801155, 'max_iter': 240}. Best is trial 1 with value: 0.5026418487340943.\n",
      "[I 2025-04-13 19:37:22,619] Trial 3 finished with value: 0.5028578902616357 and parameters: {'C': 1.801186335369927, 'max_iter': 561}. Best is trial 1 with value: 0.5026418487340943.\n",
      "[I 2025-04-13 19:37:44,289] Trial 4 finished with value: 0.502857854795234 and parameters: {'C': 1.0255581334968018, 'max_iter': 106}. Best is trial 1 with value: 0.5026418487340943.\n",
      "[I 2025-04-13 19:38:05,903] Trial 5 finished with value: 0.5026351339126053 and parameters: {'C': 24.028629028127696, 'max_iter': 715}. Best is trial 5 with value: 0.5026351339126053.\n",
      "[I 2025-04-13 19:38:30,588] Trial 6 finished with value: 0.5027577139420433 and parameters: {'C': 0.07350523855356789, 'max_iter': 1398}. Best is trial 5 with value: 0.5026351339126053.\n",
      "[I 2025-04-13 19:38:53,003] Trial 7 finished with value: 0.5028153939787733 and parameters: {'C': 0.02487824573562738, 'max_iter': 1852}. Best is trial 5 with value: 0.5026351339126053.\n",
      "[I 2025-04-13 19:39:13,960] Trial 8 finished with value: 0.5028586931029422 and parameters: {'C': 3.1242688484183883, 'max_iter': 1145}. Best is trial 5 with value: 0.5026351339126053.\n",
      "[I 2025-04-13 19:39:36,176] Trial 9 finished with value: 0.5029022620955872 and parameters: {'C': 0.49177007124994304, 'max_iter': 266}. Best is trial 5 with value: 0.5026351339126053.\n",
      "[I 2025-04-13 19:40:00,776] Trial 10 finished with value: 0.5026341741623521 and parameters: {'C': 82.58243560893463, 'max_iter': 830}. Best is trial 10 with value: 0.5026341741623521.\n",
      "[I 2025-04-13 19:40:22,531] Trial 11 finished with value: 0.5026342544493012 and parameters: {'C': 68.721179907156, 'max_iter': 710}. Best is trial 10 with value: 0.5026341741623521.\n",
      "[I 2025-04-13 19:40:47,661] Trial 12 finished with value: 0.5026341237097579 and parameters: {'C': 94.56983785439435, 'max_iter': 858}. Best is trial 12 with value: 0.5026341237097579.\n",
      "[I 2025-04-13 19:41:16,303] Trial 13 finished with value: 0.5026341086914085 and parameters: {'C': 98.82898027771526, 'max_iter': 1059}. Best is trial 13 with value: 0.5026341086914085.\n",
      "[I 2025-04-13 19:41:37,794] Trial 14 finished with value: 0.5026356214017503 and parameters: {'C': 17.548993233495825, 'max_iter': 1088}. Best is trial 13 with value: 0.5026341086914085.\n",
      "[I 2025-04-13 19:42:00,409] Trial 15 finished with value: 0.5026356183299291 and parameters: {'C': 17.57938075615925, 'max_iter': 1399}. Best is trial 13 with value: 0.5026341086914085.\n",
      "[I 2025-04-13 19:42:23,807] Trial 16 finished with value: 0.5026341142278624 and parameters: {'C': 97.21402162550474, 'max_iter': 955}. Best is trial 13 with value: 0.5026341086914085.\n",
      "[I 2025-04-13 19:42:45,788] Trial 17 finished with value: 0.502637561028307 and parameters: {'C': 8.121124901303151, 'max_iter': 1297}. Best is trial 13 with value: 0.5026341086914085.\n",
      "[I 2025-04-13 19:43:07,773] Trial 18 finished with value: 0.5026346537078208 and parameters: {'C': 37.37243867955053, 'max_iter': 1703}. Best is trial 13 with value: 0.5026341086914085.\n",
      "[I 2025-04-13 19:43:30,868] Trial 19 finished with value: 0.5026374224411216 and parameters: {'C': 8.472298695181191, 'max_iter': 975}. Best is trial 13 with value: 0.5026341086914085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training done!\n",
      "R√©sultats de log loss :\n",
      "\n",
      "{'log_loss_global': 0.51302, 'log_loss_y0': 0.51212, 'log_loss_y1': 0.64067, 'mean_proba_y1': 0.59398}\n"
     ]
    }
   ],
   "source": [
    "logreg_pipe = Pipeline([\n",
    "    ('preprocessing', data_preproc),  \n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "def objective(trial):\n",
    "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    "    )\n",
    "\n",
    "    C = trial.suggest_float(\"C\", 1e-2, 1e2, log=True)\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100, 2000)\n",
    "\n",
    "    logreg_pipe.set_params(\n",
    "        logreg__C=C,\n",
    "        logreg__penalty=\"l2\",\n",
    "        logreg__class_weight=\"balanced\",\n",
    "        logreg__max_iter=max_iter,\n",
    "        logreg__solver=\"lbfgs\",\n",
    "        logreg__random_state=42\n",
    "    )\n",
    "\n",
    "    logreg_pipe.fit(X_train_sub, y_train_sub)\n",
    "    y_proba = logreg_pipe.predict_proba(X_val)\n",
    "\n",
    "    return log_loss(y_val, y_proba)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "# üîß Appliquer les meilleurs param√®tres trouv√©s\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "logreg_pipe.set_params(\n",
    "    logreg__C=best_params[\"C\"],\n",
    "    logreg__penalty=\"l2\",\n",
    "    logreg__class_weight=\"balanced\",\n",
    "    logreg__max_iter=best_params[\"max_iter\"],\n",
    "    logreg__solver=\"lbfgs\",\n",
    "    logreg__random_state=42\n",
    ")\n",
    "\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "y_prob = logreg_pipe.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nTraining done!\")\n",
    "\n",
    "results = evaluate_log_loss_by_class(y_test, y_prob)\n",
    "print(\"R√©sultats de log loss :\\n\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(df, y_prob[:,1], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
