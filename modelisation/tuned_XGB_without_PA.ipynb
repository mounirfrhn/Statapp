{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: cannot import name 'XetAuthorizationError' from 'huggingface_hub.errors' (/Users/mounir/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/huggingface_hub/errors.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'XetAuthorizationError' from 'huggingface_hub.errors' (/Users/mounir/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/huggingface_hub/errors.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#import warnings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#warnings.filterwarnings(\"ignore\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     61\u001b[0m     CommitInfo,\n\u001b[1;32m     62\u001b[0m     CommitOperationAdd,\n\u001b[1;32m     63\u001b[0m     CommitOperationDelete,\n\u001b[1;32m     64\u001b[0m     DatasetCard,\n\u001b[1;32m     65\u001b[0m     DatasetCardData,\n\u001b[1;32m     66\u001b[0m     HfApi,\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RepoFile\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pool\n",
      "File \u001b[0;32m~/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/huggingface_hub/__init__.py:964\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    962\u001b[0m submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 964\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError importing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmod_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_commit_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     54\u001b[0m     CommitOperation,\n\u001b[1;32m     55\u001b[0m     CommitOperationAdd,\n\u001b[1;32m     56\u001b[0m     CommitOperationCopy,\n\u001b[1;32m     57\u001b[0m     CommitOperationDelete,\n\u001b[1;32m     58\u001b[0m     _fetch_files_to_copy,\n\u001b[1;32m     59\u001b[0m     _fetch_upload_modes,\n\u001b[1;32m     60\u001b[0m     _prepare_commit_payload,\n\u001b[1;32m     61\u001b[0m     _upload_lfs_files,\n\u001b[1;32m     62\u001b[0m     _upload_xet_files,\n\u001b[1;32m     63\u001b[0m     _warn_on_overwriting_operations,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inference_endpoints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceEndpoint, InferenceEndpointType\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_space_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpaceHardware, SpaceRuntime, SpaceStorage, SpaceVariable\n",
      "File \u001b[0;32m~/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/huggingface_hub/_commit_api.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EntryNotFoundError, HfHubHTTPError, XetAuthorizationError, XetRefreshTokenError\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_url\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlfs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UploadInfo, lfs_upload, post_lfs_batch_info\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'XetAuthorizationError' from 'huggingface_hub.errors' (/Users/mounir/Desktop/PROJETS/Statapp/.venv/lib/python3.12/site-packages/huggingface_hub/errors.py)"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "from sklearn.metrics import auc, roc_curve, precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "import optuna\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"criteo/FairJob\")\n",
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def print_results(X_test, y_test, model):\n",
    "    # Calcul des probabilités des prédictions\n",
    "    y_scores = model.predict_proba(X_test)[:, 1]  # Probabilité d'être classe 1\n",
    "\n",
    "    # Calcul des courbes précision-rappel\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "    # Calcul du F1-score pour chaque seuil\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)  # Évite division par zéro\n",
    "\n",
    "    # Trouver l'index du meilleur F1-score\n",
    "    best_threshold_index = np.argmax(f1_scores)\n",
    "\n",
    "    # Le tableau \"thresholds\" est plus court d'un élément → il faut ajuster l'indexation\n",
    "    if best_threshold_index == 0:  # Si le max est au début, on prend le premier seuil\n",
    "        best_threshold = thresholds[0]\n",
    "    else:\n",
    "        best_threshold = thresholds[best_threshold_index - 1]\n",
    "\n",
    "    # Précision et rappel au meilleur seuil\n",
    "    precision_optimal = precisions[best_threshold_index]\n",
    "    recall_optimal = recalls[best_threshold_index]\n",
    "    f1_score_optimal = f1_scores[best_threshold_index]\n",
    "\n",
    "    # Calcul du log loss\n",
    "    log_loss_value = log_loss(y_test, y_scores)\n",
    "\n",
    "    print(f'Best Threshold: {best_threshold}')\n",
    "    print(f'\\nPrecision at Best Threshold: {precision_optimal}')\n",
    "    print(f'Recall at Best Threshold: {recall_optimal}')\n",
    "    print(f'F1 Score at Best Threshold: {f1_score_optimal}')\n",
    "    print(f'Log Loss: {log_loss_value}')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(thresholds, precisions[1:], label='Précision', color='blue')\n",
    "    plt.plot(thresholds, recalls[1:], label='Recall', color='red')\n",
    "    plt.axvline(x=best_threshold, color='green', linestyle='--', label=\"Seuil Optimal\")\n",
    "    plt.xlabel(\"Seuil de classification\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Courbe Précision-Recall en fonction du seuil\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_log_loss_by_class(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Calcule la log loss globale, la log loss par classe, et la confiance moyenne sur les vrais positifs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Vecteur des vraies étiquettes (0 ou 1)\n",
    "    y_proba : array-like, shape (n_samples, 2)\n",
    "        Probabilités prédites (sorties de predict_proba)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "        - 'log_loss_global'\n",
    "        - 'log_loss_y0'\n",
    "        - 'log_loss_y1'\n",
    "        - 'mean_proba_y1' (moyenne des p(y=1) quand y=1)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_proba = np.array(y_proba)\n",
    "\n",
    "    return {\n",
    "        \"log_loss_global\": round(log_loss(y_true, y_proba), 5),\n",
    "        \"log_loss_y0\": round(log_loss(y_true[y_true == 0], y_proba[y_true == 0], labels=[0, 1]), 5),\n",
    "        \"log_loss_y1\": round(log_loss(y_true[y_true == 1], y_proba[y_true == 1], labels=[0, 1]), 5),\n",
    "        \"mean_proba_y1\": round(float(y_proba[y_true == 1, 1].mean()), 5)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['click','user_id','impression_id','product_id','protected_attribute'])\n",
    "y = df['click']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgb_tree_method = \"hist\"\n",
    "\n",
    "cat_cols_bin = list(X_train.columns[0:3])\n",
    "rank_col = ['rank']\n",
    "cat_cols = list(X_train.columns[4:17])\n",
    "\n",
    "data_preproc = ColumnTransformer([('multicat_encoding', TargetEncoder(target_type='binary'), cat_cols)], \n",
    "                              remainder='passthrough', force_int_remainder_cols=False)\n",
    "\n",
    "xgb_pipe = Pipeline([('preprocessing', data_preproc),\n",
    "                     ('xgb',XGBClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scale_pos_weight \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m         (\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39msum(y_train \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_train \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      3\u001b[0m     )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m      6\u001b[0m     X_train_train, X_val, y_train_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      7\u001b[0m         X_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_train\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "scale_pos_weight = (\n",
    "        (np.sum(y_train < 1) / np.sum(y_train > 0))\n",
    "    )\n",
    "\n",
    "def objective(trial):\n",
    "    X_train_train, X_val, y_train_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, stratify=y_train\n",
    "    )\n",
    "\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    min_child_weight = trial.suggest_float(\"min_child_weight\", 0.0001, 100, log=True)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1, log=True)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.1, 10, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.001, 100, log=True)\n",
    "    base_score = trial.suggest_float(\"base_score\", 1e-4, 1 - 1e-4)\n",
    "\n",
    "    \n",
    "    xgb_params = {\n",
    "        'xgb__max_depth': max_depth,\n",
    "        'xgb__min_child_weight': min_child_weight,\n",
    "        'xgb__subsample': subsample,\n",
    "        'xgb__learning_rate': learning_rate,\n",
    "        'xgb__colsample_bytree': colsample_bytree,\n",
    "        'xgb__reg_lambda': reg_lambda,\n",
    "        'xgb__gamma': gamma,\n",
    "        'xgb__base_score': base_score,\n",
    "        'xgb__scale_pos_weight': scale_pos_weight,\n",
    "    }\n",
    "    \n",
    "    xgb_pipe.set_params(**xgb_params)\n",
    "    xgb_pipe.fit(X_train_train, y_train_train)\n",
    "\n",
    "    prob_val = xgb_pipe.predict_proba(X_val)\n",
    "    \n",
    "    return log_loss(y_val, prob_val)\n",
    "\n",
    "n_trials = 2 # Increase to ~100\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "optimal_no_scale = study.best_trial\n",
    "\n",
    "print(\"\\nOptimization done\")\n",
    "\n",
    "xgb_tuned_params = {\n",
    "    'xgb__max_depth': optimal_no_scale.params[\"max_depth\"],\n",
    "    'xgb__min_child_weight': optimal_no_scale.params[\"min_child_weight\"],\n",
    "    'xgb__gamma': optimal_no_scale.params[\"gamma\"],\n",
    "    'xgb__subsample': optimal_no_scale.params[\"subsample\"],\n",
    "    'xgb__learning_rate': optimal_no_scale.params[\"learning_rate\"],\n",
    "    'xgb__colsample_bytree': optimal_no_scale.params[\"colsample_bytree\"],\n",
    "    'xgb__reg_lambda': optimal_no_scale.params[\"reg_lambda\"],\n",
    "    'xgb__base_score': optimal_no_scale.params[\"base_score\"],\n",
    "    'xgb__scale_pos_weight': scale_pos_weight,\n",
    "}\n",
    "    \n",
    "xgb_pipe.set_params(**xgb_tuned_params)\n",
    "xgb_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nEntrainement terminé\")\n",
    "\n",
    "xgb_adapted_scale_tuned = evaluate_log_loss_by_class(y_test, xgb_pipe.predict_proba(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
